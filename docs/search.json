[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Notes",
    "section": "",
    "text": "当今互联网以及人工智能最伟大的特点之一就是它的开源特性。历史上有很少的关键技术，尤其是像如今这些强大模型这样的技术，能够如此全面的向公众开放。除此之外，一些高校的课程的开放和分享，也让我们这些，没能进入Dream School的学生们，能够学习到最前沿的知识。几年前的我，无意中在网上看到了 CS自学指南。从此打开了自学的大门。不过苦于的时间和精力有限，很多课程都是囫囵吞枣。现在研究生毕业了，趁现在找工作的间隙，将所学的课程及内容重新梳理，总结。不过，自学的过程是痛苦的，全靠自制力来驱动，并且许多课程的内容都需要有一定的基础才能理解。为了更好地学习，利用这些宝贵的资源，我决定将这些课程的内容进行整理和总结。希望能帮助到更多的自学者。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#关于本站",
    "href": "index.html#关于本站",
    "title": "Course Notes",
    "section": "关于本站",
    "text": "关于本站\n本站是一个学习笔记的集合，主要内容包括计算机科学、人工智能、机器学习、深度学习等领域的课程笔记和代码实现。所有内容均为个人学习过程中整理的笔记，旨在帮助自己更好地理解和掌握相关知识，同时也希望能对其他自学者有所帮助。对于笔记中的错误和不足之处，欢迎大家提出意见和建议。\n\n\n\n\n\n\nNote\n\n\n\n本站的笔记内容，主要是由英文编写。由于时间问题，暂时没有翻译成中文的计划。如果有小伙伴愿意参与翻译工作，欢迎联系我。也欢迎大家提出意见和建议，共同完善笔记内容，为更多的自学者提供帮助。\n\n\n\n如何阅读笔记\n在学习的过程中，由于每个人的背景知识不同，对有些知识点的理解，可能有差异。为了更好地理解笔记内容，建议大家在阅读笔记时，结合自己的实际情况，进行适当的调整和补充。我会在笔记中尽量提供相关的背景知识和参考资料，以帮助大家更好地理解和掌握相关内容。同时，笔记中，不同的Callout类型，表示不同的内容和提示，具体如下：\n\n\n\n\n\n\n\nTip\n\n\n\n用于补充扩展知识或背景信息，帮助加深理解。\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n用于总结内容或提示学习方法，提升学习效率。\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n用于提醒易错点或细节陷阱，避免理解偏差。\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n用于标记课程重点、核心公式等需要特别掌握的内容。\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n用于数学推导的过程。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/Gen-AI/cmu-advanded-nlp.html",
    "href": "posts/Gen-AI/cmu-advanded-nlp.html",
    "title": "CMU: Advanced Natural Language Processing",
    "section": "",
    "text": "Course Website: Link Course Video: YouTube\nThere are several iteration of the course. This one is the latest version, mainly focus on the LLM.",
    "crumbs": [
      "Generative AI",
      "CMU: *Advanced Natural Language Processing*"
    ]
  },
  {
    "objectID": "posts/Gen-AI/ucb-deep-unsupervised.html",
    "href": "posts/Gen-AI/ucb-deep-unsupervised.html",
    "title": "CS294-158: Deep Unsupervised Learning",
    "section": "",
    "text": "Lecture Website: Link Lecture Video: YouTube",
    "crumbs": [
      "Generative AI",
      "CS294-158: *Deep Unsupervised Learning*"
    ]
  },
  {
    "objectID": "posts/Reinforcement Learning/ucb-cs285.html",
    "href": "posts/Reinforcement Learning/ucb-cs285.html",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "",
    "text": "This is a collection of notes and code for the UCB CS285: Deep Reinforcement Learning course. The course covers various topics in reinforcement learning, including policy gradients, actor-critic methods, deep Q-learning, and different exploration strategies. The course is designed to provide a comprehensive understanding of deep reinforcement learning and its applications.\nThere are total 23 lectures and 5 assignments in this course. It might 100 hours to finish the course, including the lectures and assignments.",
    "crumbs": [
      "Reinforcement Learning",
      "UCB CS285: Deep Reinforcement Learning"
    ]
  },
  {
    "objectID": "posts/Reinforcement Learning/ucb-cs285.html#lecture-01-introduction-overview",
    "href": "posts/Reinforcement Learning/ucb-cs285.html#lecture-01-introduction-overview",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "Lecture 01: Introduction & Overview",
    "text": "Lecture 01: Introduction & Overview\nLecture 01 introduces the course and provides an overview of reinforcement learning. It covers the basic concepts of reinforcement learning, including the agent-environment interaction, rewards, and policies. The lecture also discusses the different types of reinforcement learning algorithms, such as model-free and model-based methods.\nThe difference between Reinforcement Learning and Supervised Learning Algorithms is that: Reinforcement Learning not just trying to copy everything from data, but use the reward to figure out what should do in order to maximize the cumulative reward, and learned the policy.\n\nIn the Supervised Learning, the data points are assumed i.i.d., while in the reinforcement learning, the data is not i.i.d., previous output might effect the future inputs.\nIn the Supervised Learning, the ground truth outputs is known, and the algorithms is trained on the known ground truth, in the RL, the group truth is un-known, we only know the succeeded or failed (rewards)\nIn the Supervised Learning, the algorithms is trained on the fixed dataset, while in the RL, the algorithms is trained on the online data (the data is generated by the agent itself).\n\n\nWhat the different between RL and supervised learning?",
    "crumbs": [
      "Reinforcement Learning",
      "UCB CS285: Deep Reinforcement Learning"
    ]
  },
  {
    "objectID": "posts/Reinforcement Learning/ucb-cs285.html#lecture-02-supervised-learning-of-behaviors",
    "href": "posts/Reinforcement Learning/ucb-cs285.html#lecture-02-supervised-learning-of-behaviors",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "Lecture 02: Supervised Learning of Behaviors",
    "text": "Lecture 02: Supervised Learning of Behaviors\nLecture 02 covers the basics of supervised learning and how it can be used to learn behaviors in reinforcement learning. It discusses the concept of imitation learning, where an agent learns to mimic the behavior of an expert by observing its actions. The lecture also introduces the concept of behavioral cloning, where the agent learns a policy directly from the expert’s actions.",
    "crumbs": [
      "Reinforcement Learning",
      "UCB CS285: Deep Reinforcement Learning"
    ]
  },
  {
    "objectID": "posts/Reinforcement Learning/ucb-cs285.html#assignment-01-imitation-learning",
    "href": "posts/Reinforcement Learning/ucb-cs285.html#assignment-01-imitation-learning",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "Assignment 01: Imitation Learning",
    "text": "Assignment 01: Imitation Learning",
    "crumbs": [
      "Reinforcement Learning",
      "UCB CS285: Deep Reinforcement Learning"
    ]
  },
  {
    "objectID": "posts/Reinforcement Learning/ucb-cs285.html#assignment-02-policy-gradients",
    "href": "posts/Reinforcement Learning/ucb-cs285.html#assignment-02-policy-gradients",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "Assignment 02: Policy Gradients",
    "text": "Assignment 02: Policy Gradients",
    "crumbs": [
      "Reinforcement Learning",
      "UCB CS285: Deep Reinforcement Learning"
    ]
  },
  {
    "objectID": "posts/Others/stanford-cs25.html",
    "href": "posts/Others/stanford-cs25.html",
    "title": "Stanford CS25: Transformers United",
    "section": "",
    "text": "斯坦福大学的 CS25 是一门极受欢迎的研讨型课程，聚焦于 Transformer 架构及其在各个领域的前沿应用。该课程没有Assignments，主要通过阅读论文和课堂讨论来深入理解 Transformer 的原理和应用。每节课都可以独立的观看，该课程已经从 V1(2023) 更新到了 V5(2025)。 有兴趣的同学可以挑自己感兴趣的部分来学习。\n课程资源:",
    "crumbs": [
      "Others",
      "Stanford CS25: Transformers United"
    ]
  },
  {
    "objectID": "posts/Others/stanford-cs25.html#transformers-for-video-generation",
    "href": "posts/Others/stanford-cs25.html#transformers-for-video-generation",
    "title": "Stanford CS25: Transformers United",
    "section": "Transformers for Video Generation",
    "text": "Transformers for Video Generation\nLecture Video Link\n这节Lecture 主要介绍了MovieGen(Polyak et al. 2025)，以及它的一个重要的发现：\n\nScaling simple transformer architectures (data, compute, parameters) works just as well for video as it does for text.\n\n首先，第一件事就是如何学习视频的表示，在这里，他们是 Temporal Autoencoder,\n\n\n\nThe use of temporal autoencoder to represent the video.\n\n\n\n[!info]\nSpatio-Temporal Dimension is the common discription in the video. The spation mean the height and width of the Image (\\(H \\times W\\)). And the Temporal is the time \\(T\\). Morden video model, such as Video GPT, TimeSformer, are all use patio-temporal tokens.\n\nUpon use the Temporal Autoencoder, there are 8* compression across each spation-temporal dimension. For the 768px 16s 16fps video, if we model the pixel directly, it will get: more than 450M tokens, but if we use the Temporal Autoencoder, we will just get the 2M tokens, which is computationaly feasible.\n\nPre-Training Data Gather\n\n\n\nimage-20250705210218209\n\n\n\nVisual Filtering:\n\nResolution: minimum width or height of 720 px.\n\nAspect Ratio:\n\nA mix of 60% landscape and 40% portrait videos.\n\nNo Text: Use Video OCR model to remove the videos with text\n\n\n\nQ1: Why not use specialized architectures like CNNs for video tasks (e.g., action diffusion in robotics)?\nA: Specialized architectures like CNNs offer useful inductive biases for small-scale setups, but transformers scale better. At large scale, transformers can learn these biases themselves and benefit from established infrastructure. Meta found scaling transformers outperformed scaling CNNs for high-quality video generation.\n\n\n\nQ2: Can this model architecture be used for 3D generation or video game content?\nA: Yes. Once the data (e.g., 3D scenes, game environments) is tokenized into sequences, the transformer architecture becomes modality-agnostic. The key is to find a good encoding method to convert the data into discrete tokens.\n\n\n\nQ3: Why are generated videos limited to 16 seconds? What’s the challenge with longer durations?\nA: The main constraint is sequence length and computational cost. A 16-second video already produces ~73K tokens with compression. Doubling the duration doubles the token count. Potential solutions include better compression, hierarchical generation, or chunked temporal generation.\n\n\n\nQ4: What happens if we run out of clean training video data?\nA: Two strategies:\n1. Improve filtering and concept balancing pipelines, possibly with LLMs.\n2. Use generated data in post-training phases, which is already common in language modeling (e.g., RLHF).\n\n\n\nQ5: How can academic researchers contribute without access to large-scale compute for pretraining?\nA: Academia plays a vital role in proposing new training objectives, architectures, and post-training methods. For example, flow matching originated in academic work. Innovations at small scale can influence large-scale industrial projects.\n\n\n\nQ6: How do you ensure that the auto-generated captions (via LLaMA-3) are high quality?\nA: Meta trained a dedicated LLaMA-3-based video captioner. Limitations include frame subsampling (not all frames seen) and imperfect descriptions. Improving caption quality can directly enhance generation performance.\n\n\n\nQ7: What text encoders are used, and how important is the choice?\nA: MovieGen uses three frozen encoders: UL2, MetaCLIP, and T5. Surprisingly, decoder-only models (like LLaMA or GPT) perform worse for this use case. Models aligned with vision (like CLIP) work better for multimodal tasks.\n\n\n\nQ8: Can the model handle long, detailed prompts describing multi-step actions?\nA: It can handle basic sequences, but struggles with detailed, multi-event scripts. This may stem from the limited detail and sequencing in pretraining captions.\n\n\n\nQ9: Can physics priors be hardcoded to improve realism (e.g., car collisions)?\nA: The project intentionally avoids inductive biases, but injecting physics knowledge (e.g., from game engine data) could help. It’s a promising direction for future research.\n\n\n\nQ10: Is there any work on watermarking to prevent misuse like deepfakes?\nA: Yes. Meta and other groups like DeepMind are actively working on video watermarking techniques for authenticity and safety.\n\n\n\nQ11: Why include GAN discriminators in the VAE decoder?\nA: GAN losses encourage perceptual realism and allow more aggressive compression. This technique, inspired by VQGAN, helps produce sharper and more varied reconstructions than simple L1 pixel-level losses.\n\n\n\nQ12: How do you prevent fake/generated content from polluting the training data?\nA: Meta applies strict filtering to ensure high-quality real content. That said, generated data isn’t always harmful—many post-training techniques (e.g., SFT, RLHF) rely on synthetic outputs. The key is quality control.",
    "crumbs": [
      "Others",
      "Stanford CS25: Transformers United"
    ]
  },
  {
    "objectID": "posts/Gen-AI/stanford-cs336.html",
    "href": "posts/Gen-AI/stanford-cs336.html",
    "title": "Stanford CS336: Language Modeling from Scratch",
    "section": "",
    "text": "Lecture Website: Link Lecture Video: YouTube\nThis is a collection of notes and code for the Stanford CS336: Language Modeling from Scratch course. This is course is designed to teach students how to build a “large” language model (LLM) from scratch, covering from bottom to top, including the data collection, tokenization, model architecture, pre-training, post-training, inference and model evaluation. The course is designed to provide a comprehensive understanding of how LLMs work and how to build them from scratch.\nThere are total 17 Lectures and 5 Assignments in this course.",
    "crumbs": [
      "Generative AI",
      "Stanford CS336: *Language Modeling from Scratch*"
    ]
  },
  {
    "objectID": "posts/Gen-AI/stanford-cs336.html#lectures",
    "href": "posts/Gen-AI/stanford-cs336.html#lectures",
    "title": "Stanford CS336: Language Modeling from Scratch",
    "section": "Lectures",
    "text": "Lectures",
    "crumbs": [
      "Generative AI",
      "Stanford CS336: *Language Modeling from Scratch*"
    ]
  },
  {
    "objectID": "posts/Gen-AI/stanford-cs336.html#assignments",
    "href": "posts/Gen-AI/stanford-cs336.html#assignments",
    "title": "Stanford CS336: Language Modeling from Scratch",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 01: Basics\nIn this assignment, we will implement the Byte-Pair Encoding(BPE) algorithm for tokenization, and build a transformer model from scratch. And also define optimization and training loop for the model.\n\n\nAssignment 02: System\nIn this assignment, we will profile and benchmark the model we built in the assignment 01, and optimize the attention mechanism using Triton by implementing the FlashAttention2(Dao 2023).",
    "crumbs": [
      "Generative AI",
      "Stanford CS336: *Language Modeling from Scratch*"
    ]
  },
  {
    "objectID": "posts/Gen-AI/kaist-diffusion-models.html",
    "href": "posts/Gen-AI/kaist-diffusion-models.html",
    "title": "KAIST CS492(D): Diffusion Models and Their Applications",
    "section": "",
    "text": "Lecture Website: Link Lecture Video: YouTube\nTotal 17 Lectures and 7 Assignments in this course.",
    "crumbs": [
      "Generative AI",
      "KAIST CS492(D): *Diffusion Models and Their Applications*"
    ]
  }
]