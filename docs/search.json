[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Notes",
    "section": "",
    "text": "å½“ä»Šäº’è”ç½‘ä»¥åŠäººå·¥æ™ºèƒ½æœ€ä¼Ÿå¤§çš„ç‰¹ç‚¹ä¹‹ä¸€å°±æ˜¯å®ƒçš„å¼€æºç‰¹æ€§ã€‚å†å²ä¸Šæœ‰å¾ˆå°‘çš„å…³é”®æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åƒå¦‚ä»Šè¿™äº›å¼ºå¤§æ¨¡å‹è¿™æ ·çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¦‚æ­¤å…¨é¢çš„å‘å…¬ä¼—å¼€æ”¾ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸€äº›é«˜æ ¡çš„è¯¾ç¨‹çš„å¼€æ”¾å’Œåˆ†äº«ï¼Œä¹Ÿè®©æˆ‘ä»¬è¿™äº›ï¼Œæ²¡èƒ½è¿›å…¥Dream Schoolçš„å­¦ç”Ÿä»¬ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°æœ€å‰æ²¿çš„çŸ¥è¯†ã€‚å‡ å¹´å‰çš„æˆ‘ï¼Œæ— æ„ä¸­åœ¨ç½‘ä¸Šçœ‹åˆ°äº† CSè‡ªå­¦æŒ‡å—ã€‚ä»æ­¤æ‰“å¼€äº†è‡ªå­¦çš„å¤§é—¨ã€‚ä¸è¿‡è‹¦äºçš„æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œå¾ˆå¤šè¯¾ç¨‹éƒ½æ˜¯å›«å›µåæ£ã€‚ç°åœ¨ç ”ç©¶ç”Ÿæ¯•ä¸šäº†ï¼Œè¶ç°åœ¨æ‰¾å·¥ä½œçš„é—´éš™ï¼Œå°†æ‰€å­¦çš„è¯¾ç¨‹åŠå†…å®¹é‡æ–°æ¢³ç†ï¼Œæ€»ç»“ã€‚\nè‡ªå­¦çš„è¿‡ç¨‹æ˜¯ç—›è‹¦çš„ï¼Œå…¨é è‡ªåˆ¶åŠ›æ¥é©±åŠ¨ã€‚è¯·å¤§å®¶ä¸è¦æ”¾å¼ƒï¼Œæ¯•ç«ŸåšæŒä¸‹å»å°±ä¼šæœ‰æ”¶è·ã€‚å¸Œæœ›è¿™ä¸ªåšå®¢èƒ½å¸®åŠ©åˆ°ä½ ä»¬ã€‚",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#å…³äºæœ¬ç«™",
    "href": "index.html#å…³äºæœ¬ç«™",
    "title": "Course Notes",
    "section": "å…³äºæœ¬ç«™",
    "text": "å…³äºæœ¬ç«™\næœ¬ç«™æ˜¯ä¸€ä¸ªå­¦ä¹ ç¬”è®°çš„é›†åˆï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬è®¡ç®—æœºç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰é¢†åŸŸçš„è¯¾ç¨‹ç¬”è®°å’Œä»£ç å®ç°ã€‚æ‰€æœ‰å†…å®¹å‡ä¸ºä¸ªäººå­¦ä¹ è¿‡ç¨‹ä¸­æ•´ç†çš„ç¬”è®°ï¼Œæ—¨åœ¨å¸®åŠ©è‡ªå·±æ›´å¥½åœ°ç†è§£å’ŒæŒæ¡ç›¸å…³çŸ¥è¯†ï¼ŒåŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¯¹å…¶ä»–è‡ªå­¦è€…æœ‰æ‰€å¸®åŠ©ã€‚å¯¹äºç¬”è®°ä¸­çš„é”™è¯¯å’Œä¸è¶³ä¹‹å¤„ï¼Œæ¬¢è¿å¤§å®¶æå‡ºæ„è§å’Œå»ºè®®ã€‚\n\n\n\n\n\n\nNote\n\n\n\næœ¬ç«™çš„ç¬”è®°å†…å®¹ï¼Œä¸»è¦æ˜¯ç”±è‹±æ–‡ç¼–å†™ã€‚ç”±äºæ—¶é—´é—®é¢˜ï¼Œæš‚æ—¶æ²¡æœ‰ç¿»è¯‘æˆä¸­æ–‡ã€‚å¦‚æœæœ‰å°ä¼™ä¼´æ„¿æ„å‚ä¸ç¿»è¯‘å·¥ä½œï¼Œæ¬¢è¿è”ç³»æˆ‘ã€‚ä¹Ÿæ¬¢è¿å¤§å®¶æå‡ºæ„è§å’Œå»ºè®®ï¼Œå…±åŒå®Œå–„ç¬”è®°å†…å®¹ï¼Œä¸ºæ›´å¤šçš„è‡ªå­¦è€…æä¾›å¸®åŠ©ã€‚\n\n\n\nå¦‚ä½•é˜…è¯»ç¬”è®°\nåœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œç”±äºæ¯ä¸ªäººçš„èƒŒæ™¯çŸ¥è¯†ä¸åŒï¼Œå¯¹æœ‰äº›çŸ¥è¯†ç‚¹çš„ç†è§£ï¼Œå¯èƒ½æœ‰å·®å¼‚ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ç¬”è®°å†…å®¹ï¼Œå»ºè®®å¤§å®¶åœ¨é˜…è¯»ç¬”è®°æ—¶ï¼Œç»“åˆè‡ªå·±çš„å®é™…æƒ…å†µï¼Œè¿›è¡Œé€‚å½“çš„è°ƒæ•´å’Œè¡¥å……ã€‚æˆ‘ä¼šåœ¨ç¬”è®°ä¸­å°½é‡æä¾›ç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†å’Œå‚è€ƒèµ„æ–™ï¼Œä»¥å¸®åŠ©å¤§å®¶æ›´å¥½åœ°ç†è§£å’ŒæŒæ¡ç›¸å…³å†…å®¹ã€‚åŒæ—¶ï¼Œç¬”è®°ä¸­ï¼Œä¸åŒçš„Calloutç±»å‹ï¼Œè¡¨ç¤ºä¸åŒçš„å†…å®¹å’Œæç¤ºï¼Œå…·ä½“å¦‚ä¸‹ï¼š\n\n\n\n\n\n\n\nTip\n\n\n\nç”¨äºè¡¥å……æ‰©å±•çŸ¥è¯†æˆ–èƒŒæ™¯ä¿¡æ¯ï¼Œå¸®åŠ©åŠ æ·±ç†è§£ã€‚\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nç”¨äºæ€»ç»“å†…å®¹æˆ–æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nç”¨äºæé†’æ˜“é”™ç‚¹æˆ–ç»†èŠ‚é™·é˜±ï¼Œé¿å…ç†è§£åå·®ã€‚\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nç”¨äºæ ‡è®°è¯¾ç¨‹é‡ç‚¹ã€æ ¸å¿ƒå…¬å¼ç­‰éœ€è¦ç‰¹åˆ«æŒæ¡çš„å†…å®¹ã€‚\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nç”¨äºæ•°å­¦æ¨å¯¼çš„è¿‡ç¨‹ã€‚",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#è¯¾ç¨‹åˆ—è¡¨",
    "href": "index.html#è¯¾ç¨‹åˆ—è¡¨",
    "title": "Course Notes",
    "section": "è¯¾ç¨‹åˆ—è¡¨",
    "text": "è¯¾ç¨‹åˆ—è¡¨\nä»¥ä¸‹æ˜¯æˆ‘æ•´ç†çš„è¯¾ç¨‹åˆ—è¡¨ï¼ŒåŒ…å«äº†è¯¾ç¨‹åç§°ã€è®²åº§æ•°é‡ã€ä½œä¸šæ•°é‡ã€é¢„è®¡å­¦ä¹ æ—¶é—´å’Œè¯¾ç¨‹æè¿°ç­‰ä¿¡æ¯ã€‚æ¯ä¸ªè¯¾ç¨‹çš„é“¾æ¥éƒ½æŒ‡å‘äº†ç›¸åº”çš„è¯¾ç¨‹ç¬”è®°é¡µé¢ï¼Œæ–¹ä¾¿å¤§å®¶æŸ¥çœ‹è¯¦ç»†å†…å®¹ã€‚\n\n\n\n\nSchool\nCourse Name\n# of Lectures\n# of Assignments\nTime Assumed\nDescription\n\n\n\n\nStanford\nCS336: Language Modeling from Scratch\n17\n5\n200 hours\nStanfordâ€¯CS336 is an implementationâ€‘heavy course that leads students through building, optimizing, scaling, and aligning Transformer language models entirely from scratchâ€”from dataset creation to distributed GPU training and safety alignment.\n\n\nUCB\nCS285: Deep Reinforcement Learning\n23\n5\n100 hours\nThis course is an advanced course that systematically explores the foundations, algorithms, and cutting-edge research in deep reinforcement learning, covering topics from imitation learning and policy gradients to model-based RL, offline RL, and meta-learning.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/Gen-AI/kaist-diffusion-models.html",
    "href": "posts/Gen-AI/kaist-diffusion-models.html",
    "title": "KAIST CS492(D): Diffusion Models and Their Applications",
    "section": "",
    "text": "Lecture Website: Link Lecture Video: YouTube\nTotal 17 Lectures and 7 Assignments in this course.",
    "crumbs": [
      "Generative AI",
      "KAIST CS492(D): *Diffusion Models and Their Applications*"
    ]
  },
  {
    "objectID": "posts/Gen-AI/ucb-deep-unsupervised.html",
    "href": "posts/Gen-AI/ucb-deep-unsupervised.html",
    "title": "CS294-158: Deep Unsupervised Learning",
    "section": "",
    "text": "Lecture Website: Link Lecture Video: YouTube",
    "crumbs": [
      "Generative AI",
      "CS294-158: *Deep Unsupervised Learning*"
    ]
  },
  {
    "objectID": "posts/Others/stanford-cs25.html",
    "href": "posts/Others/stanford-cs25.html",
    "title": "Stanford CS25: Transformers United",
    "section": "",
    "text": "æ–¯å¦ç¦å¤§å­¦çš„ CS25 æ˜¯ä¸€é—¨æå—æ¬¢è¿çš„ç ”è®¨å‹è¯¾ç¨‹ï¼Œèšç„¦äº Transformer æ¶æ„åŠå…¶åœ¨å„ä¸ªé¢†åŸŸçš„å‰æ²¿åº”ç”¨ã€‚è¯¥è¯¾ç¨‹æ²¡æœ‰Assignmentsï¼Œä¸»è¦é€šè¿‡é˜…è¯»è®ºæ–‡å’Œè¯¾å ‚è®¨è®ºæ¥æ·±å…¥ç†è§£ Transformer çš„åŸç†å’Œåº”ç”¨ã€‚æ¯èŠ‚è¯¾éƒ½å¯ä»¥ç‹¬ç«‹çš„è§‚çœ‹ï¼Œè¯¥è¯¾ç¨‹å·²ç»ä» V1(2023) æ›´æ–°åˆ°äº† V5(2025)ã€‚ æœ‰å…´è¶£çš„åŒå­¦å¯ä»¥æŒ‘è‡ªå·±æ„Ÿå…´è¶£çš„éƒ¨åˆ†æ¥å­¦ä¹ ã€‚\nè¯¾ç¨‹èµ„æº:",
    "crumbs": [
      "Others",
      "Stanford CS25: Transformers United"
    ]
  },
  {
    "objectID": "posts/Others/stanford-cs25.html#transformers-for-video-generation",
    "href": "posts/Others/stanford-cs25.html#transformers-for-video-generation",
    "title": "Stanford CS25: Transformers United",
    "section": "Transformers for Video Generation",
    "text": "Transformers for Video Generation\nLecture Video Link\nè¿™èŠ‚Lecture ä¸»è¦ä»‹ç»äº†MovieGen(Polyak et al. 2025)ï¼Œä»¥åŠå®ƒçš„ä¸€ä¸ªé‡è¦çš„å‘ç°ï¼š\n\nScaling simple transformer architectures (data, compute, parameters) works just as well for video as it does for text.\n\né¦–å…ˆï¼Œç¬¬ä¸€ä»¶äº‹å°±æ˜¯å¦‚ä½•å­¦ä¹ è§†é¢‘çš„è¡¨ç¤ºï¼Œåœ¨è¿™é‡Œï¼Œä»–ä»¬æ˜¯ Temporal Autoencoder,\n\n\n\nThe use of temporal autoencoder to represent the video.\n\n\n\n[!info]\nSpatio-Temporal Dimension is the common discription in the video. The spation mean the height and width of the Image (\\(H \\times W\\)). And the Temporal is the time \\(T\\). Morden video model, such as Video GPT, TimeSformer, are all use patio-temporal tokens.\n\nUpon use the Temporal Autoencoder, there are 8* compression across each spation-temporal dimension. For the 768px 16s 16fps video, if we model the pixel directly, it will get: more than 450M tokens, but if we use the Temporal Autoencoder, we will just get the 2M tokens, which is computationaly feasible.\n\nPre-Training Data Gather\n\n\n\nimage-20250705210218209\n\n\n\nVisual Filtering:\n\nResolution: minimum width or height of 720 px.\n\nAspect Ratio:\n\nA mix of 60% landscape and 40% portrait videos.\n\nNo Text: Use Video OCR model to remove the videos with text\n\n\n\nQ1: Why not use specialized architectures like CNNs for video tasks (e.g., action diffusion in robotics)?\nA: Specialized architectures like CNNs offer useful inductive biases for small-scale setups, but transformers scale better. At large scale, transformers can learn these biases themselves and benefit from established infrastructure. Meta found scaling transformers outperformed scaling CNNs for high-quality video generation.\n\n\n\nQ2: Can this model architecture be used for 3D generation or video game content?\nA: Yes. Once the data (e.g., 3D scenes, game environments) is tokenized into sequences, the transformer architecture becomes modality-agnostic. The key is to find a good encoding method to convert the data into discrete tokens.\n\n\n\nQ3: Why are generated videos limited to 16 seconds? Whatâ€™s the challenge with longer durations?\nA: The main constraint is sequence length and computational cost. A 16-second video already produces ~73K tokens with compression. Doubling the duration doubles the token count. Potential solutions include better compression, hierarchical generation, or chunked temporal generation.\n\n\n\nQ4: What happens if we run out of clean training video data?\nA: Two strategies:\n1. Improve filtering and concept balancing pipelines, possibly with LLMs.\n2. Use generated data in post-training phases, which is already common in language modeling (e.g., RLHF).\n\n\n\nQ5: How can academic researchers contribute without access to large-scale compute for pretraining?\nA: Academia plays a vital role in proposing new training objectives, architectures, and post-training methods. For example, flow matching originated in academic work. Innovations at small scale can influence large-scale industrial projects.\n\n\n\nQ6: How do you ensure that the auto-generated captions (via LLaMA-3) are high quality?\nA: Meta trained a dedicated LLaMA-3-based video captioner. Limitations include frame subsampling (not all frames seen) and imperfect descriptions. Improving caption quality can directly enhance generation performance.\n\n\n\nQ7: What text encoders are used, and how important is the choice?\nA: MovieGen uses three frozen encoders: UL2, MetaCLIP, and T5. Surprisingly, decoder-only models (like LLaMA or GPT) perform worse for this use case. Models aligned with vision (like CLIP) work better for multimodal tasks.\n\n\n\nQ8: Can the model handle long, detailed prompts describing multi-step actions?\nA: It can handle basic sequences, but struggles with detailed, multi-event scripts. This may stem from the limited detail and sequencing in pretraining captions.\n\n\n\nQ9: Can physics priors be hardcoded to improve realism (e.g., car collisions)?\nA: The project intentionally avoids inductive biases, but injecting physics knowledge (e.g., from game engine data) could help. Itâ€™s a promising direction for future research.\n\n\n\nQ10: Is there any work on watermarking to prevent misuse like deepfakes?\nA: Yes. Meta and other groups like DeepMind are actively working on video watermarking techniques for authenticity and safety.\n\n\n\nQ11: Why include GAN discriminators in the VAE decoder?\nA: GAN losses encourage perceptual realism and allow more aggressive compression. This technique, inspired by VQGAN, helps produce sharper and more varied reconstructions than simple L1 pixel-level losses.\n\n\n\nQ12: How do you prevent fake/generated content from polluting the training data?\nA: Meta applies strict filtering to ensure high-quality real content. That said, generated data isnâ€™t always harmfulâ€”many post-training techniques (e.g., SFT, RLHF) rely on synthetic outputs. The key is quality control.",
    "crumbs": [
      "Others",
      "Stanford CS25: Transformers United"
    ]
  },
  {
    "objectID": "posts/Gen-AI/stanford-cs336.html",
    "href": "posts/Gen-AI/stanford-cs336.html",
    "title": "Stanford CS336: Language Modeling from Scratch",
    "section": "",
    "text": "Lecture Website: Link Lecture Video: YouTube\nThis is a collection of notes and code for the Stanford CS336: Language Modeling from Scratch course. This is course is designed to teach students how to build a â€œlargeâ€ language model (LLM) from scratch, covering from bottom to top, including the data collection, tokenization, model architecture, pre-training, post-training, inference and model evaluation. The course is designed to provide a comprehensive understanding of how LLMs work and how to build them from scratch.\nThere are total 17 Lectures and 5 Assignments in this course.",
    "crumbs": [
      "Generative AI",
      "Stanford CS336: <tag style='font-weight: bold'>Language Modeling from Scratch</tag>"
    ]
  },
  {
    "objectID": "posts/Gen-AI/stanford-cs336.html#lectures",
    "href": "posts/Gen-AI/stanford-cs336.html#lectures",
    "title": "Stanford CS336: Language Modeling from Scratch",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 01: Overview & Tokenization\n\nTokenization\nTokenization is the process of converting raw text into a sequence of tokens(usually is represented by integer indices), which are the basic units of meaning in a language model.\nIn this lecture, we will cover the basics of tokenization, including the different types of tokenization, such as character-level, word-level, and subword-level tokenization. We will also discuss the importance of tokenization in language modeling and how it affects the performance of the model.\nA Tokenizer is a class that implements the encode and decode methods for tokenization: - encode method takes a string of text and converts it into a sequence of integer indices, which are the tokens. - decode method takes a sequence of integer indices and converts it back into a string of text. The Tokenizer will generate a list of tokens from the input text, the number of the tokens is called the vocabulary size.\nAn interactive demo of the tokenization process can be found here\nThere are several tokenization algorithms, such as:\nOne of the good quantity to measure the tokenization algorithm is the compression ratio, which is defined as the ratio of the number of bytes of the input text to the number of tokens. A higher compression ratio indicates that the tokenization algorithm is more efficient in representing the input text.\ndef get_compression_ratio(string: str, indices: list[int]) -&gt; float:\n    \"\"\"\n    Calculate the compression ratio of a string and its tokenized indices.\n    \n    Args:\n        string (str): The input string.\n        indices (list[int]): The tokenized indices.\n        \n    Returns:\n        float: The compression ratio.\n    \"\"\"\n    return len(string.encode('utf-8')) / len(indices)\nThere are different types of the tokenization:\n\nCharacter-based tokenization: A Unicode string is a sequence of Unicode code points, which can be represented as a sequence of integers. Each character in the string is treated as a separate token.\n\nord('a')   # 97 \nchr(97)   # 'a'\n\nord('ğŸŒ') # 127757\nchr(127757) # 'ğŸŒ'\nOne of the main drawbacks of character-based tokenization is that it can lead to a large vocabulary size, which can make the model more difficult to train and less efficient in terms of memory usage. And some character are not commonly used in the text, which is inefficient use of the vocabulary.\n\nByte-Based Tokenization: Unicode strings can be encoded as a sequence of bytes using UTF-8 encoding. Each byte can be represented by integers in the range of 0 to 255. Some Unicode characters may be represented by multiple bytes.\n\nbytes('a', 'utf-8')  # b'a'\nbytes('ğŸŒ', 'utf-8')  # b'\\xf0\\x9f\\x8c\\x8d'\nAs we can expected, the compression ratio is 1, which is terrible.\nThe vocabulary size is 256, which is a nice property because it is fixed and covers all possible bytes. However, the main drawback is that it can lead to long sequences, since each character (even multi-byte Unicode characters) is split into multiple tokens. This can make training less efficient and increase the computational cost for language models, given that the context length of a Transformer model is limited.\n\nWord-Based Tokenization\nAnother common approach is to tokenize the text into words, where each word is treated as a separate token. We can use regular expression to split the text into words.\n\n\n\n\n\n\nTip\n\n\n\n\\w+ matches any word character, which includes:\n\nLetters: Aâ€“Z, aâ€“z\nDigits: 0â€“9\nUnderscore: _\n\n+ is a quantifier meaning â€œone or more timesâ€\n\n\nA more fancy expression is :\nGPT2_TOKENIZER_REGEX = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\nsegments = re.findall(GPT2_TOKENIZER_REGEX, string)\nAfter we get the segments, we can assign each segment to a unique integer index, which is the token. One of the main drawbacks of word-based tokenization is that the vocabulary size is not fixed and can grow very large, especially with rare or out-of-vocabulary words. This can make the model harder to train and require more memory. Additionally, it struggles with handling misspellings, new words, or languages with rich morphology. New words we havenâ€™t seen during training will be treated as UNK tokens, which can lead to a loss of information and context.\n\n\nByte Pair Encoding (BPE)\nThe basic idea of BPE is to iteratively merge the most frequent pairs of characters or subwords in the text until a desired vocabulary size is reached. This allows for a more efficient representation of the text, as it can capture common subwords and reduce the overall vocabulary size. (In the info encoding)\n\nThe GPT-2 paper used word-based tokenization to break up the text into initial segments and run the original BPE algorithm on each segment.\n\nBelow are the summary of four different tokenization algorithms:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization Type\nDescription\nCompression Ratio\nVocabulary Size\nPros\nCons\n\n\n\n\nCharacter-Based\nEach character is a token (Unicode code point).\nLow\nLarge (all characters)\nSimple; handles any language\nInefficient; large vocab; rare chars waste tokens\n\n\nByte-Based\nEach byte from UTF-8 encoding is a token (0â€“255).\n~1\nFixed (256)\nFixed vocab; language-agnostic\nLong sequences for Unicode; inefficient for modeling semantics\n\n\nWord-Based\nWords (or regex-matched units) are tokens.\nMedium\nLarge and dynamic\nIntuitive; better compression than char/byte\nPoor generalization; large vocab; OOV issues with UNK\n\n\nByte Pair Encoding (BPE)\nIteratively merges frequent subword pairs to form subword tokens.\nHigh\nMedium (tunable)\nEfficient; balances granularity and vocab size\nMerge rules are corpus-dependent; needs initial segmentation\n\n\n\n\n\nTableÂ 1: Summary of 4 Tokenization Algorithms\n\n\n\n\n\n\n\n\nLecture 02: Pytorch, Resource Accounting\nh100_bytes = 80e9\nBytes_per_parameter = 4 + 4 + ( 4+ 4) =&gt; Parameters, gradient, optimizer state\nNum Paramter = (h100_byes * 8) / Bytes_per_parameter 4e10\nx = torch.tensor([\n    [1, 2, 3],\n])\n\nMemory Account\nfloat32 is the most common data type used in deep learning, which uses 4 bytes per element.\n\n\n\n\n\n\nFigureÂ 1: The representation of float32 in memory\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe exponent used to represent the dynamic range of the number, for float32, it uses 8 bits for the exponent, which allows for a dynamic range of approximately 1.18e-38 to 3.4e+38. The fraction is used to represent the precision of the number, which is also known as resolution. For float32, it uses 23 bits for the fraction, which allows for a precision of approximately 7 decimal digits. To represent a number in float32, it uses the following formula: \\[\n\\begin{aligned}\n\\text{value}\n&= (-1)^{b_{31}} \\times 2^{(b_{30}b_{29} \\dots b_{23})_2 - 127} \\times (1.b_{22}b_{21} \\dots b_{0})_2 \\\\\n&= (-1)^{\\text{sign}} \\times 2^{(E - 127)} \\times \\left( 1 + \\sum_{i=1}^{23} b_{23 - i} \\cdot 2^{-i} \\right)\n\\end{aligned}\n\\] where sign is the sign bit, exponent is the exponent value, and fraction is the fraction value. The exponent is biased by 127, which means that the exponent value is stored as the actual exponent plus 127. This allows for both positive and negative exponents to be represented.\n\n\nMemory is determined by the number of values in the tensor and the data type of the tensor. The memory usage can be calculated as: \\[\n\\text{Memory} = \\text{Number of Values} \\times \\text{Bytes per Value}\n\\tag{1}\\] which can be calculated as:\ndef get_memory_usage(x: torch.Tensor):\n    return x.numel() * x.element_size()\nThe float32 takes 4 bytes per element, for the LLM, this is a lot of memory. We are use float16 to reduce the memory usage by half, which is 2 bytes per element. This is a common practice in deep learning to save memory and speed up training.\n\n\n\n\n\n\nFigureÂ 2: The representation of float16 in memory\n\n\n\nThe float16 cuts down the memory usage by half, but it has a smaller dynamic range and precision compared to float32. This can lead to numerical instability and loss of precision in some cases, especially when dealing with very large or very small numbers.\nx = torch.tensor([1e-8], dtype=torch.float16)  \nassert x == 0\nwhen the underflow or overflow happens, the value will be rounded to 0 or infinity, which might cause the instability in the training process.\nAnother data type is bfloat16, which is a 16-bit floating point format that has the same dynamic range as float32, but with less precision. It uses 8 bits for the exponent and 7 bits for the fraction, which allows for a dynamic range of approximately 3.4e-38 to 3.4e+38, but with a precision of only 2 decimal digits.\n\nIn conclusion, the choice of data type can have a significant impact on the memory usage and performance of the model. The float32 is the most common data type used in deep learning, but it can be memory-intensive. The float16 and bfloat16 are commonly used to reduce the memory usage, but they can lead to numerical instability and loss of precision in some cases. One of the common practice to combine float16 and float32 is to use float16 for the model parameters and gradients, and float32 for the optimizer state. This is known as mixed precision training, which can save memory and speed up training without sacrificing too much performance.\n\n\nCompute Account\nBy defaults, all the tensors are stored in the CPU memory, which is not efficient for training deep learning models. We can move the tensors to the GPU memory by calling the to method on the tensor, which will move the tensor to the specified device.\nx = torch.tensor([1, 2, 3])\nx = x.to('cuda')  # Move tensor to GPU\nOne way to check the memory usage of the GPU in PyTorch is to use the torch.cuda.memory_allocated() and torch.cuda.memory_reserved() functions. The memory_allocated() function returns the total memory allocated by the tensors, while the memory_reserved() function returns the total memory reserved by the CUDA allocator.\nThe PyTorch tensor are pointers to the memory allocated on the GPU, with metadata such as the shape, data type, and device. There are several commom operations that can be performed on the tensors:\nx = torch.tensor([[1., 2, 3], [4, 5, 6]]) \ny = x[0] # Get the first row of the tensor\nz = x[:, 1] # Get the second column of the tensor\nw = x[0, 1] # Get the element at the first row and second\n\ny = x.view(3, 2) # Reshape the tensor to 3 rows and 2 columns\nz = x.reshape(3, 2) # Reshape the tensor to 3 rows\nw = x.transpose(0, 1) # Transpose the tensor \nOne of the most important operations in deep learning is the matrix multiplication, which can be performed using the torch.matmul() function. Also, besides that, the einops library provides a powerful way to manipulate tensors, allowing for more complex operations\nNow, letâ€™s talk about the compute account, which is the number of floating point operations (FLOPs) required to perform a certain operation. The FLOPs can be calculated as the number of floating point operations divided by the time taken to perform the operation.\n\n\n\n\n\n\nNote\n\n\n\nThe FLOPs is a measure of the computational complexity of an operation, and it is often used to compare the performance of different algorithms. The FLOP/s (FLOPs per second) is a measure of the performance of a hardware, which is the number of floating point operations that can be performed in one second. It is often used to compare the performance of different hardware, such as CPUs and GPUs.\n\n\nSupport we have a Linear layer with B batch size, D input dimension, and K output dimension. The number of floating point operations required to perform the forward pass of the linear layer can be calculated as:\nFLOPs = 2 * B * D * K\nwhere 2 is the number of floating point operations required to perform the matrix multiplication and addition (1 for multiplication and 1 for addition) of (i, j, k) triple.\nBeside the matrix multiplication, the element-wise operations such as activation functions (e.g., ReLU, sigmoid) also contribute to the FLOPs. For example, if we apply a ReLU activation function after the linear layer, it will add B * K additional floating point operations, since it applies the function to each element in the output tensor. Addition of two \\(m \\times n\\) matrices requires \\(mn\\) FLOPs, where \\(m\\) is the number of rows and \\(n\\) is the number of columns.\nFLOPs is measure in terms of the number of the floating point operations required to perform a certain operation, which is a common way to measure the computational complexity of an algorithm. In theory, how to map those FLOPs to the wall clock time is not straightforward, since it depends on the hardware and the implementation of the algorithm. However, we can use the following formula to estimate the wall clock time:\nWall Clock Time = FLOPs / FLOP/s\nOne of the criterion is the MFU(Model FLOPs Utilization).Usually, MFU of &gt;= 0.5 is quite good (and will be higher if matmuls dominate)\nFloating point operation is a basic operation like addition or multiplication.\nForward: 2 * B * D * K (#tokens) (#parameters)\nFLOPs =&gt; Wall Clock time.\n\n\n\nLecture 03: Architectures & Hyperparameters\n\n\nLecture 04: Mixture of Experts\n\n\nLecture 05 & 06: GPU, Kernels, Triton\n\n\nLecture 07 & 08: Parallelism\n\n\nLecture 10: Inference\n\n\nLecture 09 & 11: Scaling Laws\n\n\nLecture 12: Evaluation\n\n\nLecture 13 & 14: Data\n\n\nLecture 15, 16 & 17: Alignment: SFT/ RLHF",
    "crumbs": [
      "Generative AI",
      "Stanford CS336: <tag style='font-weight: bold'>Language Modeling from Scratch</tag>"
    ]
  },
  {
    "objectID": "posts/Gen-AI/stanford-cs336.html#assignments",
    "href": "posts/Gen-AI/stanford-cs336.html#assignments",
    "title": "Stanford CS336: Language Modeling from Scratch",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 01: Basics\nIn this assignment, we will implement the Byte-Pair Encoding(BPE) algorithm for tokenization, and build a transformer model from scratch. And also define optimization and training loop for the model.\n\n\nAssignment 02: System\nIn this assignment, we will profile and benchmark the model we built in the assignment 01, and optimize the attention mechanism using Triton by implementing the FlashAttention2(Dao 2023).",
    "crumbs": [
      "Generative AI",
      "Stanford CS336: <tag style='font-weight: bold'>Language Modeling from Scratch</tag>"
    ]
  },
  {
    "objectID": "posts/RL/ucb-cs285.html",
    "href": "posts/RL/ucb-cs285.html",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "",
    "text": "This is a collection of notes and code for the UCB CS285: Deep Reinforcement Learning course. The course covers various topics in reinforcement learning, including policy gradients, actor-critic methods, deep Q-learning, and different exploration strategies. The course is designed to provide a comprehensive understanding of deep reinforcement learning and its applications.\nThere are total 23 lectures and 5 assignments in this course. It might 100 hours to finish the course, including the lectures and assignments."
  },
  {
    "objectID": "posts/RL/ucb-cs285.html#lectures",
    "href": "posts/RL/ucb-cs285.html#lectures",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 01: Introduction & Overview\nLecture 01 introduces the course and provides an overview of reinforcement learning. It covers the basic concepts of reinforcement learning, including the agent-environment interaction, rewards, and policies. The lecture also discusses the different types of reinforcement learning algorithms, such as model-free and model-based methods.\nThe difference between Reinforcement Learning and Supervised Learning Algorithms is that: Reinforcement Learning not just trying to copy everything from data, but use the reward to figure out what should do in order to maximize the cumulative reward, and learned the policy.\n\nIn the Supervised Learning, the data points are assumed i.i.d., while in the reinforcement learning, the data is not i.i.d., previous output might effect the future inputs.\nIn the Supervised Learning, the ground truth outputs is known, and the algorithms is trained on the known ground truth, in the RL, the group truth is un-known, we only know the succeeded or failed (rewards)\nIn the Supervised Learning, the algorithms is trained on the fixed dataset, while in the RL, the algorithms is trained on the online data (the data is generated by the agent itself).\n\n\n\nLecture 02: Supervised Learning of Behaviors\nLecture 02 covers the basics of supervised learning and how it can be used to learn behaviors in reinforcement learning. It discusses the concept of imitation learning, where an agent learns to mimic the behavior of an expert by observing its actions. The lecture also introduces the concept of behavioral cloning, where the agent learns a policy directly from the expertâ€™s actions."
  },
  {
    "objectID": "posts/RL/ucb-cs285.html#assignments",
    "href": "posts/RL/ucb-cs285.html#assignments",
    "title": "UCB CS285: Deep Reinforcement Learning",
    "section": "Assignments",
    "text": "Assignments\nThere are total of 5 assignments in this course, from imitation learning to deep Q-Learning. The assignments are designed to help you understand the concepts and algorithms in reinforcement learning, and to implement them in code.\n\nAssignment 01: Imitation Learning\n\n\nAssignment 02: Policy Gradients"
  }
]