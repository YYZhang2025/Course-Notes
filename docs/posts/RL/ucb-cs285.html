<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuyang Zhang">

<title>UCB CS285: Deep Reinforcement Learning ‚Äì Learning Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../posts/Gen-AI/kaist-diffusion-models.html" rel="next">
<link href="../../index.html" rel="prev">
<link href="../.././images/icon.avif" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-cc542ae9cbb2dc3da5b9f84cb8966572.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-0fd53820e9cb11da4fdcad6c7683cbd0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-cc542ae9cbb2dc3da5b9f84cb8966572.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<style>
div.callout-summary.callout {
  border-left-color: #4b8bbe;
}
div.callout-summary.callout-style-default > .callout-header {
  background-color: rgba(75, 139, 190, 0.13);
}
div.callout-summary .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-summary.callout-style-default .callout-icon::before, div.callout-summary.callout-titled .callout-icon::before {
  content: 'üìñ';
  background-image: none;
}
div.callout-QA.callout {
  border-left-color: #e7f3ff;
}
div.callout-QA.callout-style-default > .callout-header {
  background-color: rgba(231, 243, 255, 0.13);
}
div.callout-QA .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-QA.callout-style-default .callout-icon::before, div.callout-QA.callout-titled .callout-icon::before {
  content: 'üó®Ô∏è';
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../style/style.css">
<meta property="og:title" content="UCB CS285: Deep Reinforcement Learning ‚Äì Learning Notes">
<meta property="og:description" content="">
<meta property="og:image" content="./ucb-cs285.assets/dagger-alg.png">
<meta property="og:site_name" content="Learning Notes">
<meta property="og:image:alt" content="DAgger Algorithm">
</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/RL/ucb-cs285.html">Reinforcement Learning</a></li><li class="breadcrumb-item"><a href="../../posts/RL/ucb-cs285.html">UCB CS285: <tag style="font-weight: bold">Deep Reinforcement Learning</tag></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../.././images/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://yyzhang2000.github.io/Blog/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-globe"></i></a>
    <a href="https://github.com/YYZhang2025" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/zhang-yuyang/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <a href="mailto:zhangyuyang1211@gmail.com" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-envelope"></i></a>
    <a href="https://noblecatt-1304922865.cos.ap-singapore.myqcloud.com/Yuyang_CV_General.pdf" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-file-person"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/RL/ucb-cs285.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">UCB CS285: <tag style="font-weight: bold">Deep Reinforcement Learning</tag></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Generative AI</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Gen-AI/kaist-diffusion-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KAIST CS492(D): <em>Diffusion Models and Their Applications</em></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Gen-AI/stanford-cs336.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stanford CS336: <tag style="font-weight: bold">Language Modeling from Scratch</tag></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Gen-AI/ucb-deep-unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CS294-158: <em>Deep Unsupervised Learning</em></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Others</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Others/stanford-cs25.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stanford CS25: Transformers United</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#terminology-notation" id="toc-terminology-notation" class="nav-link active" data-scroll-target="#terminology-notation">Terminology &amp; Notation</a></li>
  <li><a href="#lecture-01-introduction-overview" id="toc-lecture-01-introduction-overview" class="nav-link" data-scroll-target="#lecture-01-introduction-overview"><span class="header-section-number">1</span> Lecture 01: Introduction &amp; Overview</a>
  <ul>
  <li><a href="#what-is-reinforcement-learning" id="toc-what-is-reinforcement-learning" class="nav-link" data-scroll-target="#what-is-reinforcement-learning"><span class="header-section-number">1.0.1</span> What is Reinforcement Learning?</a></li>
  <li><a href="#credit-assignment-problem" id="toc-credit-assignment-problem" class="nav-link" data-scroll-target="#credit-assignment-problem"><span class="header-section-number">1.0.2</span> Credit Assignment Problem</a></li>
  <li><a href="#rl-in-ai-systemslanguage-vision" id="toc-rl-in-ai-systemslanguage-vision" class="nav-link" data-scroll-target="#rl-in-ai-systemslanguage-vision"><span class="header-section-number">1.0.3</span> RL in AI Systems(Language &amp; Vision)</a></li>
  <li><a href="#suttons-the-bitter-lesson" id="toc-suttons-the-bitter-lesson" class="nav-link" data-scroll-target="#suttons-the-bitter-lesson"><span class="header-section-number">1.0.4</span> Sutton‚Äôs ‚ÄúThe Bitter Lesson‚Äù</a></li>
  <li><a href="#rl-beyond-reward-maximization" id="toc-rl-beyond-reward-maximization" class="nav-link" data-scroll-target="#rl-beyond-reward-maximization"><span class="header-section-number">1.0.5</span> RL Beyond Reward Maximization</a></li>
  <li><a href="#leveraging-pre-trained-models" id="toc-leveraging-pre-trained-models" class="nav-link" data-scroll-target="#leveraging-pre-trained-models"><span class="header-section-number">1.0.6</span> Leveraging Pre-trained Models</a></li>
  <li><a href="#challenges-in-reinforcement-learning" id="toc-challenges-in-reinforcement-learning" class="nav-link" data-scroll-target="#challenges-in-reinforcement-learning"><span class="header-section-number">1.0.7</span> Challenges in Reinforcement Learning</a></li>
  </ul></li>
  <li><a href="#lecture-02-supervised-learning-of-behaviors-imitation-learning" id="toc-lecture-02-supervised-learning-of-behaviors-imitation-learning" class="nav-link" data-scroll-target="#lecture-02-supervised-learning-of-behaviors-imitation-learning"><span class="header-section-number">2</span> Lecture 02: Supervised Learning of Behaviors (Imitation Learning)</a>
  <ul>
  <li><a href="#behavioral-cloning" id="toc-behavioral-cloning" class="nav-link" data-scroll-target="#behavioral-cloning"><span class="header-section-number">2.1</span> Behavioral Cloning</a></li>
  <li><a href="#math-explain-of-behavioral-cloning" id="toc-math-explain-of-behavioral-cloning" class="nav-link" data-scroll-target="#math-explain-of-behavioral-cloning"><span class="header-section-number">2.2</span> Math explain of Behavioral Cloning</a></li>
  <li><a href="#dagger-algorithm" id="toc-dagger-algorithm" class="nav-link" data-scroll-target="#dagger-algorithm"><span class="header-section-number">2.3</span> DAgger Algorithm</a></li>
  </ul></li>
  <li><a href="#lecture-03-introduction-to-pytorch" id="toc-lecture-03-introduction-to-pytorch" class="nav-link" data-scroll-target="#lecture-03-introduction-to-pytorch"><span class="header-section-number">3</span> Lecture 03: Introduction to PyTorch</a></li>
  <li><a href="#lecture-04-introduction-to-reinforcement-learning" id="toc-lecture-04-introduction-to-reinforcement-learning" class="nav-link" data-scroll-target="#lecture-04-introduction-to-reinforcement-learning"><span class="header-section-number">4</span> Lecture 04: Introduction to Reinforcement Learning</a></li>
  <li><a href="#lecture-05-policy-gradient" id="toc-lecture-05-policy-gradient" class="nav-link" data-scroll-target="#lecture-05-policy-gradient"><span class="header-section-number">5</span> Lecture 05: Policy Gradient</a>
  <ul>
  <li><a href="#gradient-evaluation" id="toc-gradient-evaluation" class="nav-link" data-scroll-target="#gradient-evaluation"><span class="header-section-number">5.1</span> Gradient Evaluation</a></li>
  <li><a href="#reduce-variance" id="toc-reduce-variance" class="nav-link" data-scroll-target="#reduce-variance"><span class="header-section-number">5.2</span> Reduce Variance</a>
  <ul>
  <li><a href="#baseline-technique" id="toc-baseline-technique" class="nav-link" data-scroll-target="#baseline-technique"><span class="header-section-number">5.2.1</span> Baseline Technique</a></li>
  </ul></li>
  <li><a href="#off-policy-policy-gradient" id="toc-off-policy-policy-gradient" class="nav-link" data-scroll-target="#off-policy-policy-gradient"><span class="header-section-number">5.3</span> Off-Policy Policy Gradient</a>
  <ul>
  <li><a href="#important-sampling" id="toc-important-sampling" class="nav-link" data-scroll-target="#important-sampling"><span class="header-section-number">5.3.1</span> Important Sampling</a></li>
  <li><a href="#importance-sampled-policy-gradient" id="toc-importance-sampled-policy-gradient" class="nav-link" data-scroll-target="#importance-sampled-policy-gradient"><span class="header-section-number">5.3.2</span> Importance Sampled Policy Gradient</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#lecture-06-actor-critic-methods" id="toc-lecture-06-actor-critic-methods" class="nav-link" data-scroll-target="#lecture-06-actor-critic-methods"><span class="header-section-number">6</span> Lecture 06: Actor-Critic Methods</a></li>
  <li><a href="#assignments" id="toc-assignments" class="nav-link" data-scroll-target="#assignments"><span class="header-section-number">7</span> Assignments</a>
  <ul>
  <li><a href="#assignment-01-imitation-learning" id="toc-assignment-01-imitation-learning" class="nav-link" data-scroll-target="#assignment-01-imitation-learning"><span class="header-section-number">7.1</span> Assignment 01: Imitation Learning</a></li>
  <li><a href="#assignment-02-policy-gradients" id="toc-assignment-02-policy-gradients" class="nav-link" data-scroll-target="#assignment-02-policy-gradients"><span class="header-section-number">7.2</span> Assignment 02: Policy Gradients</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../posts/RL/ucb-cs285.html">Reinforcement Learning</a></li><li class="breadcrumb-item"><a href="../../posts/RL/ucb-cs285.html">UCB CS285: <tag style="font-weight: bold">Deep Reinforcement Learning</tag></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">UCB CS285: <tag style="font-weight: bold">Deep Reinforcement Learning</tag></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yuyang Zhang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>This is a collection of notes and code for the <a href="https://rail.eecs.berkeley.edu/deeprlcourse/">UCB CS285: Deep Reinforcement Learning</a> course. The course covers various topics in reinforcement learning, including <strong>policy gradients</strong>, <strong>actor-critic methods</strong>, <strong>deep Q-learning</strong>, and <strong>different exploration strategies</strong>. The course is designed to provide a comprehensive understanding of deep reinforcement learning and its applications.</p>
<p>There are total <tag style="color: orange">23 lectures</tag> and <tag style="color: orange">5 assignments</tag> in this course. It might <tag style="color: red">100 hours</tag> to finish the course, including the lectures and assignments.</p>
<section id="terminology-notation" class="level2 unnumbered page-columns page-full">
<h2 class="unnumbered anchored" data-anchor-id="terminology-notation">Terminology &amp; Notation</h2>
<div id="tbl-terminology" class="column-page hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-terminology-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table">
<colgroup>
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 41%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Symbol</th>
<th>Definition</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Observation</strong></td>
<td><span class="math inline">\(o_t\)</span></td>
<td>What the agent perceives from the environment (e.g., camera image).</td>
<td>May be partial or noisy; does not always uniquely determine the state.</td>
</tr>
<tr class="even">
<td><strong>Action</strong></td>
<td><span class="math inline">\(a_t\)</span></td>
<td>Decision taken by the agent at time <span class="math inline">\(t\)</span>.</td>
<td>Can be discrete (e.g., ‚Äúleft‚Äù, ‚Äúright‚Äù) or continuous (e.g., steering angle).</td>
</tr>
<tr class="odd">
<td><strong>Policy</strong></td>
<td><span class="math inline">\(\pi_\theta\)</span></td>
<td>Mapping from observation (or state) to action (or distribution over actions).</td>
<td><span class="math inline">\(\theta\)</span> are the learnable parameters (e.g., neural network weights).</td>
</tr>
<tr class="even">
<td><strong>State</strong></td>
<td><span class="math inline">\(s_t\)</span></td>
<td>True, complete description of the environment at time <span class="math inline">\(t\)</span>.</td>
<td>Underlies the observation; satisfies the Markov property.</td>
</tr>
<tr class="odd">
<td><strong>Markov Property</strong></td>
<td><span class="math inline">\(\begin{gather*}
P(s_{t+1} \mid s_t, a_t) \\
= P(s_{t+1} \mid s_{&lt;t}, a_{&lt;t})
\end{gather*}\)</span></td>
<td>Future state depends only on current state and action: <span class="math inline">\(P(s_{t+1} \mid s_t, a_t)\)</span>.</td>
<td>Defines the concept of a state in decision processes.</td>
</tr>
<tr class="even">
<td><strong>Behavioral Cloning</strong></td>
<td><span class="math inline">\(\min_\theta , \mathbb{E}{(s_t, a_t) \sim \mathcal{D}} \left[ \mathcal{L} \left( \pi\theta(s_t), a_t \right) \right]\)</span></td>
<td>Supervised learning method to mimic expert behavior by training on <span class="math inline">\((o_t, a_t)\)</span> pairs.</td>
<td>Can suffer from compounding error due to distribution shift.</td>
</tr>
<tr class="odd">
<td><strong>Stochastic Policy</strong></td>
<td><span class="math inline">\(a_t \sim \pi(a_t \mid o_t)\)</span></td>
<td>Outputs a probability distribution over actions.</td>
<td>More general and often preferred for optimization.</td>
</tr>
<tr class="even">
<td><strong>Deterministic Policy</strong></td>
<td><span class="math inline">\(a_t = \pi_\theta(o_t)\)</span></td>
<td>Always outputs the same action for a given input.</td>
<td>Special case of a stochastic policy.</td>
</tr>
<tr class="odd">
<td><strong>Transition Model</strong></td>
<td><span class="math inline">\(P(s_{t+1} \mid s_t, a_t)\)</span></td>
<td>Defines how the environment evolves from state to state.</td>
<td>Captures system dynamics (e.g., physics).</td>
</tr>
<tr class="even">
<td><strong>DAgger</strong></td>
<td>‚Äì</td>
<td>Imitation learning algorithm that mitigates compounding error by iterative expert feedback.</td>
<td>Will be implemented in homework and discussed in Part 2.</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl quarto-uncaptioned" id="tbl-terminology-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1
</figcaption>
</figure>
</div>
<p>There are total of <u>23 lectures</u> in this course, each lecture covers a specific topic in reinforcement learning. The Lecture 01 introduces the course and provides an overview of reinforcement learning, while the subsequent lectures delve into specific algorithms and techniques.</p>
</section>
<section id="lecture-01-introduction-overview" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Lecture 01: Introduction &amp; Overview</h1>
<p>Lecture 01 introduces the course and provides an overview of reinforcement learning. It covers the basic concepts of reinforcement learning, including:</p>
<ul>
<li>the agent-environment interaction,</li>
<li>rewards</li>
<li>policies</li>
</ul>
<div id="vid-robot-grasping" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-robot-grasping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RikmTF3ZNWo?si=HpEyJx3FPXuguN3W" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-robot-grasping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Example of Robot Grasping
</figcaption>
</figure>
</div>
<p>It first introduce an example of Robotic Grasping (<a href="#vid-robot-grasping" class="quarto-xref">Video&nbsp;1</a>). In this example, the goal is to <u>build a systems for robotic object grasping</u>. This is non-trivial problem, because of object variability, such as rigid, deformable, and articulated objects. In this case, <em>manually designing every special case is not feasible, so we need to use machine learning algorithms to solve this problem</em>. Instead of hand-engineered logic, use can use ML (e.g., CNNs) to <u>learn grasp strategies from data</u>. However, the in the traditional ML algorithms, we requires a bunch of labeled data, which is hard to obtain in the real world. So, is there a way to learn from the data without the labels? The answer is yes, we can use Reinforcement Learning (RL) to solve this problem.</p>
<p>RL allows robots to learn from <strong>trial and error</strong>, rather than human labels. To be more specific, <u>instead of providing a the correct action for each state, we give the robot a reward signal (e.g., success / failure), which tells the robot how well it is doing</u>. The robot can then use this reward signal to learn a <strong>policy</strong> that <u>maximizes the cumulative reward over time</u>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Recent AI Advances and Limitations">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recent AI Advances and Limitations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Until recently (2023), massive progress in generative models: DALL-E, Stable Diffusion, GPT, etc. These model use <strong>supervised learning</strong> or <strong>density estimation</strong> (e.g., <span class="math inline">\(P(x), P(y|x)\)</span>) on human-generated data. This is effective when the goal is to replicate human-like data/ behavior. In this context, we just learning the distribution of the dataset in different fancy ways, so it‚Äôs only as good as the data we have, which means it mimic human outputs, but may not generalize well or innovate.</p>
</div>
</div>
<p>With the development of deep learning and hardware, RL has become a powerful tool for solving complex problems. When combines large-scale optimization (e.g., deep nets) with RL principles, we can build systems that can learn to solve complex tasks, and might emerge new capabilities that were not explicitly programmed ‚Äì e.g.&nbsp;AlphaGo‚Äôs famous ‚Äúmove 37‚Äù.</p>
<section id="what-is-reinforcement-learning" class="level3" data-number="1.0.1">
<h3 data-number="1.0.1" class="anchored" data-anchor-id="what-is-reinforcement-learning"><span class="header-section-number">1.0.1</span> What is Reinforcement Learning?</h3>
<p>RL is both a mathematical formalism and an approach for learning decision-making from experience. It distnguish between the problem (sequential decision-making) and the solution (learning a policy). One main difference between RL and other machine learning approaches is the data:</p>
<ul>
<li>Supervised learning assumes I.I.D. labeled data <span class="math inline">\((X, Y)\)</span></li>
<li>RL involves sequential, temporally correlated data and often lacks explicit labels ‚Äì only feedback in terms of rewards.</li>
</ul>
</section>
<section id="credit-assignment-problem" class="level3" data-number="1.0.2">
<h3 data-number="1.0.2" class="anchored" data-anchor-id="credit-assignment-problem"><span class="header-section-number">1.0.2</span> Credit Assignment Problem</h3>
<p>The credit assignment problem is a fundamental challenge in reinforcement learning. It refers to the difficulty of determining which actions in a sequence of actions are responsible for the eventual outcome or reward. In other words, when an agent receives a reward after taking a series of actions, it must figure out which of those actions contributed to the reward and which did not. This is particularly challenging in environments where the effects of actions are delayed or not immediately observable. This illustrates the need for learning from sequences of events and delayed feedback.</p>
<p>The lecture also discusses the different types of reinforcement learning algorithms, such as model-free and model-based methods.</p>
</section>
<section id="rl-in-ai-systemslanguage-vision" class="level3" data-number="1.0.3">
<h3 data-number="1.0.3" class="anchored" data-anchor-id="rl-in-ai-systemslanguage-vision"><span class="header-section-number">1.0.3</span> RL in AI Systems(Language &amp; Vision)</h3>
<p>With the ChatGPT and other large language models, we can see the power of RL in AI systems. RL is used to fine-tune the models to align with human preferences and to improve their performance on specific tasks. For example, RL can be used to optimize the responses of a language model to make them more coherent and relevant to the user‚Äôs query. Similarly, in computer vision, RL can be used to train models to recognize objects and scenes more accurately by providing feedback based on the model‚Äôs predictions.</p>
<p>The RL also plays a crucial role in Image Generation: RL optimizes prompts-to-image pipelines using caption feedback.</p>
</section>
<section id="suttons-the-bitter-lesson" class="level3" data-number="1.0.4">
<h3 data-number="1.0.4" class="anchored" data-anchor-id="suttons-the-bitter-lesson"><span class="header-section-number">1.0.4</span> Sutton‚Äôs ‚ÄúThe Bitter Lesson‚Äù</h3>
<p>Richard Sutton argues that learning and search (not manual design) are the scalable paths to intelligence. Scalable learning machines outperform systems engineered by intuition. Learning helps extract patterns; search / optimization helps generate new behaviors. Both necessary for flexible, rational decision-making in real-world problems.</p>
<p>All ML problems (vision, control, language) can be reframed as decision-making problems. For example:</p>
<ul>
<li>Vision: Recognizing objects in images is a decision-making problem where the agent must decide which features to focus on and how to classify the objects.</li>
<li>Control: Controlling a robot is a decision-making problem where the agent must decide which actions to take based on the current state of the environment.</li>
<li>Language: Generating text is a decision-making problem where the agent must decide which words to use and how to structure the sentences to convey the intended meaning.</li>
</ul>
<p>Thus, RL applies broadly‚Äîeven supervised learning is often RL in disguise.</p>
</section>
<section id="rl-beyond-reward-maximization" class="level3" data-number="1.0.5">
<h3 data-number="1.0.5" class="anchored" data-anchor-id="rl-beyond-reward-maximization"><span class="header-section-number">1.0.5</span> RL Beyond Reward Maximization</h3>
<p>RL is not just about maximizing rewards, the real-world RL also tackles:</p>
<ul>
<li>Reward Inference (inverse RL): Learning the reward function from observed behavior.</li>
<li>Imitation Learning: Learning from expert demonstrations without explicit rewards.</li>
<li>Transfer / Meta Learning: Adapting learned policies to new tasks or environments.</li>
<li>Prediction and Planning: Using learned models to predict future states and plan actions.</li>
</ul>
</section>
<section id="leveraging-pre-trained-models" class="level3" data-number="1.0.6">
<h3 data-number="1.0.6" class="anchored" data-anchor-id="leveraging-pre-trained-models"><span class="header-section-number">1.0.6</span> Leveraging Pre-trained Models</h3>
<p>RT-2 <span class="citation" data-cites="RT2VisionLanguageActionModels2023brohan">(<a href="#ref-RT2VisionLanguageActionModels2023brohan" role="doc-biblioref">Brohan et al. 2023</a>)</span> combines VLMs with robot control to complete complex tasks by grounding instructions in internet knowledge and robot experience.</p>
<div class="callout callout-style-default callout-note callout-titled" title="The summary of RT-2: Vision-Language Action Models">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The summary of RT-2: Vision-Language Action Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>RT‚Äë2 examines how to merge internet‚Äëscale vision‚Äëlanguage (VLM) knowledge with low‚Äëlevel robotic control, enabling robots to generalize to new objects, environments, and even execute reasoning commands‚Äîfar beyond the limited distribution of robotics data alone.</p>
<p>Co‚Äëfine‚Äëtuning strategy: they train a pre‚Äëtrained VLM (e.g.&nbsp;PaLI‚ÄëX or PaLM‚ÄëE) simultaneously on: - Robotic trajectory data (visual frames paired with robot actions), - Internet‚Äëscale vision‚Äëlanguage tasks (like VQA)</p>
<p>They convert low‚Äëlevel robot actions into discrete text tokens, effectively treating action sequences as if they were normal language output‚Äîso the model‚Äôs output space covers both natural language and action vocabulary</p>
</div>
</div>
<p>The difference between Reinforcement Learning and Supervised Learning Algorithms is that: <em>Reinforcement Learning not just trying to copy everything from data, but use the reward to figure out what should do in order to maximize the cumulative reward, and learned the policy.</em></p>
<ul>
<li>In the Supervised Learning, the data points are assumed <strong>i.i.d.</strong>, while in the reinforcement learning, the data is not i.i.d., <em>previous output might effect the future inputs</em>.</li>
<li>In the Supervised Learning, <em>the ground truth outputs is known</em>, and the algorithms is trained on the known ground truth, in the RL, <em>the group truth is un-known</em>, we only know the succeeded or failed (rewards)</li>
<li>In the Supervised Learning, the algorithms is trained on the <strong>fixed dataset</strong>, while in the RL, the algorithms is trained on the <strong>online data</strong> (the data is generated by the agent itself).</li>
</ul>
</section>
<section id="challenges-in-reinforcement-learning" class="level3" data-number="1.0.7">
<h3 data-number="1.0.7" class="anchored" data-anchor-id="challenges-in-reinforcement-learning"><span class="header-section-number">1.0.7</span> Challenges in Reinforcement Learning</h3>
<p>Reinforcement learning faces several challenges, including:</p>
<ol type="1">
<li>Scaling RL to Real-World Settings
<ul>
<li>While deep learning scales well, RL still struggles to scale to large, open-world, high-dimensional problems (e.g., robotics, autonomous driving).</li>
<li>Data inefficiency is a major bottleneck.</li>
</ul></li>
<li>Sparse and Delayed Rewards
<ul>
<li>Many real-world tasks (e.g., completing a PhD) have rewards that are rare or highly delayed, making it difficult for RL to learn solely from these signals.</li>
</ul></li>
<li>Designing Reward Functions:
<ul>
<li>In real tasks (e.g., pouring water), defining a precise reward signal is often as hard as solving the task itself.</li>
<li>Perception and control are tightly intertwined.</li>
</ul></li>
</ol>
<p>These challenges frame the motivation for deeper research into more sample-efficient, generalizable, and scalable reinforcement learning algorithms‚Äîespecially those that better mimic how intelligent agents (like humans) learn from limited data and rich interactions.</p>
</section>
</section>
<section id="lecture-02-supervised-learning-of-behaviors-imitation-learning" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Lecture 02: Supervised Learning of Behaviors (Imitation Learning)</h1>
<p>Lecture 02 covers the basics of supervised learning and how it can be used to learn behaviors in reinforcement learning. It discusses the concept of <strong>imitation learning</strong>, where an<u> agent learns to mimic the behavior of an expert by observing its actions</u>. The lecture also introduces the concept of <strong>behavioral cloning</strong>, where the <u>agent learns a policy directly from the expert‚Äôs actions</u>.</p>
<div class="callout callout-style-default callout-caution callout-titled" title="Behavioral Cloning vs. Imitation Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Behavioral Cloning vs.&nbsp;Imitation Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Behavioral Cloning is a specific form of Imitation Learning</em> that <u>treats policy learning as a supervised learning problem using expert demonstrations</u>, while Imitation Learning more broadly includes methods like inverse reinforcement learning and DAgger.</p>
</div>
</div>
<p>Policies as Supervised Learning Models:</p>
<ul>
<li>Policies map observations <span class="math inline">\(o_t\)</span> to actions <span class="math inline">\(a_t\)</span>., analogous to supervised learning models how classifiers map <span class="math inline">\(x \to y\)</span>.</li>
<li>Policies can be <strong>deterministic</strong> (<span class="math inline">\(a_t = \pi(o_t)\)</span>) or <strong>stochastic</strong> (<span class="math inline">\(a_t \sim \pi(a_t \mid o_t)\)</span>).</li>
</ul>
<p>Training is done on the expert distribution <span class="math inline">\(P_{\text{data}}(o_t)\)</span>, but execution happens under the learned policy‚Äôs distribution <span class="math inline">\(P_{\pi_\theta}(o_t)\)</span>, causing distributional shift, a central issue in imitation learning.</p>
<p>State vs.&nbsp;Observation:</p>
<ul>
<li><strong>State</strong>: Complete description of the environment at time <span class="math inline">\(t\)</span>.</li>
<li><strong>Observation</strong>: What the agent perceives (e.g., camera image), which may be partial or noisy, which not always sufficient to infer the state.</li>
</ul>
<p>One of the main assumption in the RL is Markov Property, which states that the future state depends only on the current state and action, not on the previous states or actions.</p>
<p><span id="eq-markov-property"><span class="math display">\[
P(s_{t+1} \mid s_t, a_t) = P(s_{t+1} \mid s_{&lt;t}, a_{&lt;t})
\tag{1}\]</span></span></p>
<p>The input to the policy is the observation or state, and the output is the action or distribution over actions. The policy can be deterministic or stochastic, depending on whether it outputs a single action or a probability distribution over actions.</p>
<section id="behavioral-cloning" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="behavioral-cloning"><span class="header-section-number">2.1</span> Behavioral Cloning</h2>
<p><strong>Behavioral cloning is a supervised learning approach to imitation learning</strong>, where the <u>agent learns a policy by directly mimicking the actions of an expert.</u> The agent is trained on pairs of observations and actions from the expert‚Äôs behavior <span class="math inline">\(\mathcal{D}(o_t, a_t)\)</span>, using supervised learning techniques, linear regression, neural networks, etc. The goal is to minimize the difference between the agent‚Äôs actions and the expert‚Äôs actions.</p>
<p>However, one of the main problems of the behavioral cloning is the <strong>compounding error</strong>: small prediction errors will lead to unseen states, which can cause the agent to diverge from the expert‚Äôs behavior. This is because the agent is trained on the expert‚Äôs actions, but when it makes a mistake, it may not be able to recover from it, leading to a snowball effect of errors.</p>
<p>This is called <strong>distribution shift</strong>: Training is done on the expert distribution <span class="math inline">\(P_{\text{data}}(o_t)\)</span>, but execution happens under the learned policy‚Äôs distribution <span class="math inline">\(P_{\pi_\theta}(o_t)\)</span>, causing distributional shift, a central issue in imitation learning.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is the not the problem in the supervised learning, because in the supervised learning, the data is i.i.d., the output of the model is not dependent on the previous inputs, and will not affect the future inputs. However, in the behavioral cloning, the data is not i.i.d., and the model is trained on the expert‚Äôs actions, which may not be the same as the actions taken by the agent during execution.</p>
</div>
</div>
<p>There are several ways to mitigate the compounding error in behavioral cloning, such as:</p>
<ul>
<li><strong>Data Augmentation</strong>: Adding noise to the observations or actions to make the agent more robust to small errors.</li>
<li><strong>Data Collection</strong>: Collecting more data from the expert to cover more states and actions.</li>
<li><strong>More Powerful Models</strong>: Using more powerful models (e.g., deep neural networks) to better approximate the expert‚Äôs policy.</li>
<li><strong>DAgger</strong>: An iterative algorithm that combines behavioral cloning with online learning, where the agent collects data from its own policy and queries the expert for corrections. This helps the agent to recover from mistakes and improve its performance over time.</li>
</ul>
</section>
<section id="math-explain-of-behavioral-cloning" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="math-explain-of-behavioral-cloning"><span class="header-section-number">2.2</span> Math explain of Behavioral Cloning</h2>
</section>
<section id="dagger-algorithm" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="dagger-algorithm"><span class="header-section-number">2.3</span> DAgger Algorithm</h2>
<p>DAgger(Dataset Aggregation) <span class="citation" data-cites="ReductionImitationLearning2011ross">(<a href="#ref-ReductionImitationLearning2011ross" role="doc-biblioref">Ross, Gordon, and Bagnell 2011</a>)</span> aims to solve the distributional shift problem in behavioral cloning by <u>iteratively collecting data from the current policy and querying the expert for corrections.</u> The key idea is to change the data collection strategy to better reflect the states the policy actually visits, rather than just improving the policy. It is an iterative process that:</p>
<div class="columns quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="column quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ol type="1">
<li>Start with an initial policy <span class="math inline">\(\pi_\theta\)</span> trained on expert data.<br>
</li>
<li>At each iteration:<br>
‚ÄÉ‚ÄÉ1. Run the current policy to collect new observations from the environment.<br>
‚ÄÉ‚ÄÉ2. Get expert labels (actions) for these new observations.<br>
‚ÄÉ‚ÄÉ3. Aggregate the new data with the previous dataset.<br>
</li>
<li>Repeat until convergence.<br>
</li>
<li>Over time, the dataset distribution converges to the test-time distribution (<span class="math inline">\(p_{\pi_\theta}\)</span>).</li>
</ol>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-dagger" class="column quarto-float quarto-figure quarto-figure-center anchored" style="width:100%;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dagger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="./ucb-cs285.assets/dagger-alg.png" alt="DAgger Algorithm" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dagger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: DAgger Algorithm
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="lecture-03-introduction-to-pytorch" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Lecture 03: Introduction to PyTorch</h1>
</section>
<section id="lecture-04-introduction-to-reinforcement-learning" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Lecture 04: Introduction to Reinforcement Learning</h1>
</section>
<section id="lecture-05-policy-gradient" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Lecture 05: Policy Gradient</h1>
<p>let first review the object of the RL. The object of RL algorithm is to maximize the expected cumulative reward under some policy <span class="math inline">\(\pi\)</span>, which is by paramter <span class="math inline">\(\theta\)</span>. The optimal <span class="math inline">\(\theta\)</span> is defined as:</p>
<p><span class="math display">\[
\theta^* = \arg\max_\theta \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=1}^{T} r(s_t, a_t) \right]  
\]</span></p>
<p>where <span class="math inline">\(\tau\)</span> is a trajectory (sequence of states, actions, rewards), and <span class="math inline">\(r(s_t, a_t)\)</span> is the reward at time <span class="math inline">\(t\)</span>. And we define <span class="math inline">\(J(\theta)\)</span> as:</p>
<p><span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=1}^{T} r(s_t, a_t) \right]
\]</span></p>
<p>We can evaluate the <span class="math inline">\(J(\theta)\)</span> using <strong>Monte Carlo methods</strong>, by sampling trajectories (rollouts) from the policy <span class="math inline">\(\pi_\theta\)</span> and computing the average return.</p>
<p><span class="math display">\[
J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} r(s_t, a_t)
\]</span></p>
<p>And the Policy Gradient methods means, we<u> update the parameters of the policy according to the information of the gradient of <span class="math inline">\(J(\theta)\)</span></u>. But, first we need to compute the gradient <span class="math inline">\(\nabla J(\theta)\)</span>.</p>
<section id="gradient-evaluation" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="gradient-evaluation"><span class="header-section-number">5.1</span> Gradient Evaluation</h2>
<p>The objective is defined as</p>
<p><span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[r(\tau)\right ] = \int p_\theta(\tau) r(\tau) d\tau
\]</span></p>
<p>So, the gradient can be computed as:</p>
<p>$$ <span class="math display">\[\begin{align*}

\nabla_\theta J(\theta)
&amp;= \nabla_\theta \int p_\theta(\tau) r(\tau) d\tau \\
&amp;= \int \nabla_\theta p_\theta(\tau) r(\tau) d\tau \quad  \hfill \text{(Leibniz integral rule)} \\
&amp;= \int p_\theta(\tau) \nabla_\theta \log p_\theta(\tau) r(\tau) d\tau \quad \text{(Log Gradient Trick)} \\
&amp;= \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \nabla_\theta \log p_\theta(\tau) r(\tau) \right] \\
&amp;= \mathbb{E}_{\tau \sim p_\theta(\tau)} \left\{ \nabla_\theta\left[ \cancel{\log p(s_1)} + \sum_{t=1}^{T} \bigg(\log \pi_\theta(a_t | s_t) + \cancel{ \log p(s_{t+1} | s_t, a_t)}\bigg) \right] r(\tau) \right\} \\
&amp;= \mathbb{E}_{\tau \sim p_\theta(\tau)} \left\{ \nabla_\theta\left[ \sum_{t=1}^{T} \log \pi_\theta(a_t | s_t)  \right]  r(\tau)\right\} \\
&amp;\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) r(\tau) \\
&amp;\approx \frac{1}{N} \sum_{i=1}^{N} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \right] \left[  \sum_{t=1}^{T} r(s_t, a_t) \right]

\end{align*}\]</span> $$</p>
<p>There are lot thing going on, let‚Äôs break this down step by step.</p>
<ol type="1">
<li><p>First, the <strong>Leibniz integral rule</strong> allows us to differentiate under the integral sign, which is a powerful technique in calculus. In our case, it lets us move the gradient operator inside the integral when computing the gradient of the expected return.</p></li>
<li><p>Next, we apply the <strong>Log Gradient Trick</strong>, which is a key insight in reinforcement learning. It allows us to express the gradient of the expected return in terms of the policy‚Äôs log probabilities. This is useful because it connects the policy‚Äôs parameters directly to the reward signal, enabling us to compute the policy gradient.</p></li>
</ol>
<div class="callout callout-style-default callout-QA callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why can we just take the gradient of the probability but on the log-probability?
</div>
</div>
<div class="callout-body-container callout-body">
<p>The probability of the trajectory <span class="math inline">\(p_\theta(\tau)\)</span> is not just depends on <span class="math inline">\(\pi_\theta\)</span>, but also on the dynamics of the environment, <span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span>. Sometimes we don‚Äôt have access to the true dynamics, and we need to rely on the policy‚Äôs log probabilities to compute the gradient.</p>
</div>
</div>
<ol start="3" type="1">
<li>Finally, we arrive at the empirical estimate of the policy gradient by averaging over multiple trajectories. This is typically done using <strong>Monte Carlo sampling</strong>, where we collect a batch of trajectories and compute the average gradient across them.</li>
</ol>
<p>When we get the gradient of the objective function, we can use it to update the policy parameters in the direction that increases the expected return. This is the <strong>REINFORCE</strong> algorithm.</p>
<div id="fig-reinforce-algorithm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reinforce-algorithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./assets/REINFORCE_alg.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reinforce-algorithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: 3 Steps of the REINFORCE Algorithm
</figcaption>
</figure>
</div>
<p>As we can see, the gradient of the objective function is very similar as the gradient of the loss function we used in supervised learning. For example, in the behavior cloning algorithms, the policy is updated to maximize the likelihood of the actions taken in the expert demonstrations(true actions). And we want to maximize the likelihood of the actions taken in the expert demonstrations(true actions). So, the object is <span class="math display">\[
J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \right]
\]</span></p>
<p>So the policy gradient can be view as weighted version of MLE by returns, and the loss is weighted cross entropy. This means, if the action is good(high returns), we want to increase its probability (via gradient ascent by increasing the derivative of the log probability), and if the action is bad(low returns), we want to decrease its probability.</p>
<p>One good thing is that the Markov property is not used in the policy gradient methods, which makes them more flexible and applicable to a wider range of problems. we can use them in partially observable environments as well.</p>
</section>
<section id="reduce-variance" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="reduce-variance"><span class="header-section-number">5.2</span> Reduce Variance</h2>
<p>However, one problem of this naive policy gradient algorithm is high variance in the gradient estimates. It‚Äôs dig into why this happens. The first thing to note is that the gradient is based on the Monte Carlo Estimation, which means the returns can vary significantly between different trajectories, leading to noisy gradient estimates.The next is the every action‚Äôs gradient is multiplied by the total return <span class="math inline">\(R(\tau)\)</span>, even if the action had little effect on that return, which lead to high variance(Credit Assignment Problem). Third, in the finite sampling estimate, if the returns are positive, all the actions will be ‚Äúreinforced‚Äù, even those with little impact on the return.</p>
<p>According to those problems, we can use the following methods to solve the problem:</p>
<ol type="1">
<li><p><strong>Add Causality</strong>: One approach to reduce variance is to incorporate causal information into the policy gradient estimates. This can be done by using techniques such as counterfactual reasoning or causal inference to better understand the impact of actions on future states and rewards.</p></li>
<li><p><strong>Baseline</strong>: One common approach is to subtract a baseline from the returns before computing the gradient. This baseline can be the average return or a value function estimate. By reducing the variance of the returns, we can obtain more stable gradient estimates.</p></li>
</ol>
<p>Let‚Äôs dig into the details of the baseline technique.</p>
<section id="baseline-technique" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="baseline-technique"><span class="header-section-number">5.2.1</span> Baseline Technique</h3>
<p>The baseline technique involves subtracting a baseline value from the returns before computing the policy gradient. This baseline can be the average return or a value function estimate. The key idea is that by reducing the variance of the returns, we can obtain more stable gradient estimates. Why we can substract the basedline from the graidnet estimation?</p>
<p>The reason we can subtract the baseline from the gradient estimation is that it does not change the expected value of the gradient. Mathematically, we can express the policy gradient as:</p>
<p><span class="math display">\[
\nabla J(\theta) = \mathbb{E} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
\]</span></p>
<p>If we introduce a baseline <span class="math inline">\(b(s_t)\)</span>, we can rewrite the policy gradient as:</p>
<p><span class="math display">\[
\begin{align}
\nabla J(\theta)
&amp;= \mathbb{E}_{\tau \sim p_\theta} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) (R(\tau) - b) \right] \\
&amp;= \mathbb{E}_{\tau \sim p_\theta} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]  - \mathbb{E}_{\tau \sim p_\theta} \left[\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t)  b  \right] \\
&amp;= \mathbb{E}_{\tau \sim p_\theta} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]  -  \int p_\theta(\tau) \nabla_\theta \log p_\theta(a_t | s_t) b d\tau \\
&amp;= \mathbb{E}_{\tau \sim p_\theta} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]  -  b \nabla_\theta \int  p_\theta(\tau) d\tau \\
&amp;= \mathbb{E}_{\tau \sim p_\theta} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]  -  b \nabla_\theta 1 \\
&amp;= \mathbb{E}_{\tau \sim p_\theta} \left[ \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]  -  0 \\
\end{align}
\]</span></p>
<p>Since the baseline is a function of the state and does not depend on the action taken, it does not affect the expected value of the gradient. By subtracting the baseline, we can reduce the variance of the gradient estimates without introducing bias. So, when subtracting the baseline, we are essentially centering the returns around the baseline, which helps to stabilize the learning process. But, which baseline should we use? let‚Äôs derive the best one.</p>
<p>We want to minimize the variance of the estimator, for now, we use <span class="math inline">\(g(\tau)\)</span> to represent <span class="math inline">\(\log \pi_\theta(a_t | s_t)\)</span>. So, the variance of the estimator can be expressed as: <span class="math display">\[
\begin{align}
\text{Var}[g(\tau)(r(\tau) - b)]
&amp;= \mathbb{E}[g(\tau)^2(r(\tau) - b)^2] - \mathbb{E}[g(\tau)(r(\tau) - b)]^2 \\
&amp;= \mathbb{E}[g(\tau)^2(r(\tau) - b)^2] - \mathbb{E}[g(\tau)r(\tau)]^2 + \mathbb{E}[g(\tau)b]^2 \\
&amp;= \mathbb{E}[g(\tau)^2(r(\tau) - b)^2] - \mathbb{E}[g(\tau)r(\tau)]^2 + 0
\end{align}
\]</span></p>
<p>So, we can see, to minish the variance, we need to minimize <span class="math inline">\(\mathbb{E}[g(\tau)r(\tau)]\)</span>.</p>
<p><span class="math display">\[
E[g^2(r‚àíb)^2]‚Äã=E[g^2(r^2‚àí2rb+b^2)]=E[g^2r^2]‚àí2bE[g^2r]+b^2E[g^2]‚Äã
\]</span></p>
<p>Take the derivative with respect to <span class="math inline">\(b\)</span> and set it to zero to find the optimal baseline:</p>
<p><span class="math display">\[
\frac{d}{db} E[g^2r^2]‚àí2bE[g^2r]+b^2E[g^2]‚Äã = 0
‚àí2E[g^2r]+2bE[g^2] = 0
\]</span> So, the optimal baseline is given by:</p>
<p><span class="math display">\[
b^* = \frac{E[g^2r]}{E[g^2]}
\]</span></p>
<p>However, in practice, we barely used the optimal baseline. The choice of baseline can significantly impact the performance of the policy gradient algorithm. In many cases, a simple and computationally efficient baseline, such as the average return or a running estimate of the value function, is used instead of the optimal baseline.</p>
</section>
</section>
<section id="off-policy-policy-gradient" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="off-policy-policy-gradient"><span class="header-section-number">5.3</span> Off-Policy Policy Gradient</h2>
<p>The REINFORCE algorithm is an on-policy algorithm, because they require sampling trajectories from the current policy at each update step, the old samples cannot be reused, because the policy is constantly being updated. And we know that neural networks require many small gradient steps, each step demands fresh samples. So, on-policy make the training very costly if the generate samples are not sufficient. So, off-policy methods have been proposed to address this issue by using the old samples to update the current policy. Before we dive into off-policy methods, let‚Äôs first understand how to get samples of one distribution from another through important sampling.</p>
<section id="important-sampling" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="important-sampling"><span class="header-section-number">5.3.1</span> Important Sampling</h3>
<p>Important sampling is a technique used to estimate the properties of a particular distribution while only having samples generated from a different distribution. It is particularly useful in reinforcement learning when we want to evaluate or optimize a policy using data collected from another policy.</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}_{x \sim P(x)}[f(x)]
&amp;= \int \frac{Q(x)}{Q(x)} P(x) f(x) dx \\
&amp;= \int Q(x) \frac{P(x)}{Q(x)} f(x) dx \\
&amp;= \mathbb{E}_{x \sim Q(x)}\left[\frac{P(x)}{Q(x)} f(x)\right]
\end{align}
\]</span></p>
<p>where <span class="math inline">\(P(x)\)</span> is the target distribution, <span class="math inline">\(Q(x)\)</span> is the proposal distribution, and <span class="math inline">\(f(x)\)</span> is the function we want to evaluate, and <span class="math inline">\(\frac{P(x)}{Q(x)}\)</span> is the importance sampling ratio, also known as important weight.</p>
</section>
<section id="importance-sampled-policy-gradient" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="importance-sampled-policy-gradient"><span class="header-section-number">5.3.2</span> Importance Sampled Policy Gradient</h3>
<p>To apply the important sampling on the policy gradient, we can rewrite the policy gradient as:</p>
<p><span class="math display">\[
\begin{align}
J(\theta)
&amp;= \mathbb{E}_{\tau \sim \tilde{p}(\tau)} \left[ \frac{P(\tau)}{\tilde{P}(\tau)} r(\tau) \right] \\
&amp;= \mathbb{E}_{\tau \sim \tilde{p}(\tau)} \left[ \frac{\cancel{p(s_1)} \prod_{t=1}^T \pi_\theta(a_t|s_t) \cancel{\prod_{t=1}^T p(s_t|s_{t-1},a_{t-1})}}{\cancel{p(s_1)}\prod_{t=1}^T \pi_\tilde{\theta}(a_t|s_t) \cancel{\prod_{t=1}^T p(s_t|s_{t-1},a_{t-1})}} r(\tau) \right] \\
&amp;= \mathbb{E}_{\tau \sim \tilde{p}(\tau)} \left[ \prod_{t=1}^T \frac{\pi_\theta(a_t|s_t)}{\pi_\tilde{\theta}(a_t|s_t)} r(\tau) \right]
\end{align}
\]</span></p>
<p>So, the gradient of the objective function become:</p>
<p><span class="math display">\[
\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau \sim \tilde{p}(\tau)} \left[ \prod_{t=1}^T \frac{\pi_\theta(a_t|s_t)}{\pi_\tilde{\theta}(a_t|s_t)}  \nabla_{\theta} \log \pi_\theta(a_t|s_t) r(\tau) \right]
\]</span></p>
<p>As we can see, if <span class="math inline">\(\theta = \tilde{\theta}\)</span>, then the importance sampling ratio becomes 1, and we recover the standard policy gradient.</p>
<p>If the importance sampling ratio is very different from 1, it can lead to high variance in the gradient estimate. This is known as the ‚Äúimportance sampling problem‚Äù and is a key challenge in off-policy reinforcement learning. One way it to apply causality:</p>
<p><span class="math display">\[
\begin{align}
\nabla J(\theta)
&amp;= \mathbb{E}_{\tau \sim p_\theta(\tau)} \Bigg[
\left( \prod_{t=1}^T \frac{\pi_{\theta'}(a_t \mid s_t)}{\pi_\theta(a_t \mid s_t)} \right)
\left( \sum_{t=1}^T \nabla_{\theta'} \log \pi_{\theta'}(a_t \mid s_t) \right)
\left( \sum_{t=1}^T r(s_t, a_t) \right)
\Bigg] \\
&amp;= \mathbb{E}_{\tau \sim p_\theta(\tau)}
\Bigg[
\left( \sum_{t=1}^T \nabla_{\theta'} \log \pi_{\theta'}(a_t \mid s_t) \right)
\left( \prod_{t=1}^T \frac{\pi_{\theta'}(a_t \mid s_t)}{\pi_\theta(a_t \mid s_t)} \right)
\left( \sum_{t=1}^T r(s_t, a_t) \right)
\Bigg]
\end{align}
\]</span></p>
</section>
</section>
</section>
<section id="lecture-06-actor-critic-methods" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Lecture 06: Actor-Critic Methods</h1>
</section>
<section id="assignments" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Assignments</h1>
<p>There are total of 5 <u>assignments</u> in this course, from imitation learning to deep Q-Learning. The assignments are designed to help you understand the concepts and algorithms in reinforcement learning, and to implement them in code.</p>
<section id="assignment-01-imitation-learning" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="assignment-01-imitation-learning"><span class="header-section-number">7.1</span> Assignment 01: Imitation Learning</h2>
</section>
<section id="assignment-02-policy-gradients" class="level2" data-number="7.2">




</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">7.2 Assignment 02: Policy Gradients</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-RT2VisionLanguageActionModels2023brohan" class="csl-entry" role="listitem">
Brohan, Anthony, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, et al. 2023. <span>‚Äú<span>RT-2</span>: <span>Vision-Language-Action Models Transfer Web Knowledge</span> to <span>Robotic Control</span>.‚Äù</span> July 28, 2023. <a href="https://doi.org/10.48550/arXiv.2307.15818">https://doi.org/10.48550/arXiv.2307.15818</a>.
</div>
<div id="ref-ReductionImitationLearning2011ross" class="csl-entry" role="listitem">
Ross, Stephane, Geoffrey J. Gordon, and J. Andrew Bagnell. 2011. <span>‚ÄúA <span>Reduction</span> of <span>Imitation Learning</span> and <span>Structured Prediction</span> to <span>No-Regret Online Learning</span>.‚Äù</span> March 16, 2011. <a href="https://doi.org/10.48550/arXiv.1011.0686">https://doi.org/10.48550/arXiv.1011.0686</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../index.html" class="pagination-link" aria-label="About">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">About</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../posts/Gen-AI/kaist-diffusion-models.html" class="pagination-link" aria-label="KAIST CS492(D): *Diffusion Models and Their Applications*">
        <span class="nav-page-text">KAIST CS492(D): <em>Diffusion Models and Their Applications</em></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>¬© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ‚ù§Ô∏è and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>